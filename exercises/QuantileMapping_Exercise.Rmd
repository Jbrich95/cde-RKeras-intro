---
title: "European rainfall GNN exercise"
author: "Jordan Richards"
date: "`r Sys.Date()`"
output: beamer_presentation
---
  
# Dataset
  
* The data consists of:
  * hourly summer rainfall (mm) over 500 grid-boxes (0.25deg by 0.25deg) across Western Europe (2021-2022).
  * Collection of 11 relevant meteorological and orographical covariates at each site.
* Obtained from ERA5: https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels
* We will use a GNN model to map high quantiles of hourly rainfall, as well as estimate the probability of extreme rainfall at a single site.
* We will treat the centroid of a  grid-box as a point location.

# Load data

```{r,eval=T}
load("../Data/EuroRain.Rdata")
dim(Y)
dim(X)
dim(coords)
```
# Observation

```{r, eval=TRUE, echo =F}
par(mfrow=c(1,2))
time.ind <-100
plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
     main = "Hourly rainfall (mm)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(Y[time.ind,])[i]],pch=20)
time.ind <-4000

plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
      main = "Hourly rainfall (mm)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(Y[time.ind,])[i]],pch=20)
``` 
# Predictors

* The predictors are:
  * meteorological: air temperature at 2m, mean sea level pressure, surface pressure, ozone, 10m u- and v-wind speed components
  * orographical: angle, isotropy, land-sea mask, slope, standard deviation
  
```{r,eval=T}
print(cov_names[1:6])
print(cov_names[7:11])
```

# Observations

```{r, eval=TRUE, echo =F}
par(mfrow=c(1,2))
time.ind <-100
plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
     main = "Temperature (K)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(X[time.ind,,1])[i]],pch=20)

plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
      main = "u-Wind (m/s)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(X[time.ind,,6])[i]],pch=20)
``` 

# Data normalisation

We first scale the input data to improve the numerical stability of training

```{r, eval=TRUE}
#Normalise inputs
X_scaled <- X
for(i in 1:dim(X)[3]){
  temp <- X[,,i]
  m <- mean( temp)
  s <- sd( temp)
  temp <- ( temp-m)/s
  X_scaled[,,i] <- temp
}

```

# Get some validation data

```{r, eval=TRUE}
#Normalise inputs
set.seed(1)
validation.inds <- sample(1:nrow(Y), nrow(Y)/5) #20%
X.valid <- X_scaled[validation.inds,,]
Y.valid <-  Y[validation.inds,]
X.train <- X_scaled[-validation.inds,,]
Y.train <-  Y[-validation.inds,]

```

# Response

We focus on estimating the $\tau=90\%$ quantile of $Y|(\mathbf{X}=\mathbf{x})$, which is hourly rainfall. This can be done using the tilted loss $l(y,\hat{y})=\max\{\tau(y-\hat{y}),(1-\tau)(y-\hat{y})\}$.

```{r, eval=TRUE}
quant.level<-0.9
tilted_loss <- function( y_true, y_pred) {
    K <- backend()
    
    error = y_true - y_pred
    return(
      100*K$mean(K$maximum(quant.level*error, 
                            (quant.level-1)*error))
      )
  }
  
```

# Building a graph

We build a graph for our data using the coordinates. The graph structure will be stored in an adjacency matrix, $A$. This sparse matrix will have entries 1 if sites are within 75km of each other, and 0 otherwise. 

```{r, eval=TRUE}
dist<-fields::rdist.earth(coords,miles=F)
cut.off.dist <- 75
A <- dist
A[dist>cut.off.dist]=0; A[dist <= cut.off.dist]=1
diag(A)=0
```
Note that we are not including self-loops.

# Building a graph

And here's a plot of the underlying graph.
```{r, eval=TRUE, echo = FALSE}
plot(coords,xlab="",ylab="",ylim=c(42,63),xlim=range(-11,6),pch=20,asp=1/1.6)
for(i in 1:500) for(j in 1:500) if(A[i,j]>0) points(coords[c(i,j),],type="l",col="red")
maps::map("world",add=T)
```

# Graph layers
First, import the spektral python package.
```{r, eval=TRUE, results = 'asis'}
spk <<- reticulate::import("spektral")
print(names(spk$layers)[1:9])
```

We can create a custom Keras layer with a spektral layer embedded within. Here we used a graph convolutional layer with a trainable skip connection. 

See also https://graphneural.network/layers/convolution/

# Custom graph layers

```{r, eval=TRUE, echo=F}
layer_graph_conv <- function(
     object,
     channels,
     activation = NULL,
     use_bias = TRUE,
     kernel_initializer = 'glorot_uniform',
     bias_initializer = 'zeros',
     kernel_regularizer = NULL,
     bias_regularizer = NULL,
     activity_regularizer = NULL,
     kernel_constraint = NULL,
     bias_constraint = NULL,
     name=NULL,
     ...)
{
  args <- list(channels = as.integer(channels),
               activation = activation,
               use_bias = use_bias,
               kernel_initializer = kernel_initializer,
               bias_initializer = bias_initializer,
               kernel_regularizer = kernel_regularizer,
               bias_regularizer = bias_regularizer,
               activity_regularizer = activity_regularizer,
               kernel_constraint = kernel_constraint,
               bias_constraint = bias_constraint,
               name=name
  )
  keras::create_layer(spk$layers$GCSConv, object, args)

}
```

```{r, eval=FALSE}
layer_graph_conv <- function(
     object,
     channels,
     activation = NULL,
     use_bias = TRUE,
     kernel_initializer = 'glorot_uniform',
     bias_initializer = 'zeros',
     kernel_regularizer = NULL,
     bias_regularizer = NULL,
     activity_regularizer = NULL,
     kernel_constraint = NULL,
     bias_constraint = NULL,
     name=NULL,
     ...)
{... #REPLACE WITH FOLLOWING}
```

# Custom graph layers (2)

```{r, eval=FALSE}

  args <- list(channels = as.integer(channels),
           activation = activation,
           use_bias = use_bias,
           kernel_initializer = kernel_initializer,
           bias_initializer = bias_initializer,
           kernel_regularizer = kernel_regularizer,
           bias_regularizer = bias_regularizer,
           activity_regularizer = activity_regularizer,
           kernel_constraint = kernel_constraint,
           bias_constraint = bias_constraint,
           name=name
  )
  keras::create_layer(spk$layers$GCSConv, object, args)


```

# Keras GNN

We can now build a Keras model as we have done previously, just by swapping layer_dense (or equivalent) with layer_graph_conv. 

However, we need an extra input to the graph layer that includes information about the graph itself - some function of the adjacency matrix. This extra input depends on the type of layer. For GCGSconv, we need the normalised adjacency matrix.

```{r, eval=T}
A.normed<-spk$utils$convolution$normalized_adjacency(A)
```
For a standard GCNN layer, we would use the modified Laplacian.

# Keras GNN

```{r, eval=T}
library(keras)

input.lay <- layer_input(shape = dim(X)[2:3])

hidden.lay1<- list(input.lay, A.normed) %>% 
  layer_graph_conv(channels = 32,
                   activation = "relu") 

hidden.lay2<- list(hidden.lay1, A.normed) %>% 
  layer_graph_conv(channels = 32,
                   activation = "relu") 


```

# Keras GNN
I want strictly positive quantiles only, so let's apply an exponential transformation in the final layer.
```{r, eval=T}
output.lay<- hidden.lay2 %>% 
  layer_dense(units = 1,   activation = "exponential") 

 model <- keras_model(
    inputs = c(input.lay), 
    outputs = c(output.lay)
  )
```

# Compiling

We will compile the model with the adam optimiser and our quantile loss function.

```{r,eval=T}
model %>% compile(
  loss = tilted_loss,
  optimizer = "adam"
)
```

# Fitting the model

I'll also shuffle the training data to break up any temporal dependence. This will help the training procedure.

```{r, eval = T}
history <- model %>% fit(
  x = X.train, y = Y.train, shuffle=T,
  epochs = 300, batch_size = 32, verbose=0,
  validation_data = list(X.valid, Y.valid),
  callbacks =  list(
    callback_model_checkpoint(
      filepath = "model1_weights",
      verbose=0, monitor="val_loss",
      save_best_only = TRUE, 
      save_weights_only = TRUE),
      callback_early_stopping(
      monitor = "val_loss", 
      min_delta = 0, patience = 20)
    )
)
```

# Load saved weights

```{r, eval = T}
model <- load_model_weights_tf(model,
                               filepath="model1_weights")
```

# Fitting the model
Check for overfitting

```{r, eval=TRUE, fig.height = 3, fig.width = 7}

plot(history)

```

# Predictions


```{r, eval=TRUE}
qhat <- model %>% predict(X_scaled)

#Sanity check
mean(qhat[,,1] > Y)

```

# Weighted adjacency

Let's try weighting the adjacency matrix, and see if that provides better predictions. We will use a Gaussian weighting kernel with range 50km.

```{r, eval=TRUE}
A2 <- exp(-(dist/50)^2)
diag(A2) <- 0
A2[dist > cut.off.dist] <- 0
A2.normed<-spk$utils$convolution$normalized_adjacency(A2)

```

# New model

This model utilises the weighted graph structure.

```{r, eval=T}
input.lay2 <- layer_input(shape = dim(X)[2:3])
hidden.lay21<- list(input.lay2, A2.normed) %>% 
  layer_graph_conv(channels = 32,
                   activation = "relu") 
hidden.lay22<- list(hidden.lay21, A2.normed) %>% 
  layer_graph_conv(channels = 32, 
                   activation = "relu") 
output.lay2 <- hidden.lay22 %>% 
  layer_dense(units=1, 
                   activation = 'exponential')
model2 <- keras_model(
    inputs = c(input.lay2), 
    outputs = c(output.lay2)
  )
```

# Compile

```{r, eval=T}
model2 %>% compile(
          loss = tilted_loss,
          optimizer = "adam"
)
```

# Fitting


```{r, eval = T}
history2 <- model2 %>% fit(
  x = X.train, y = Y.train, shuffle=T,
  epochs = 300, batch_size = 32, verbose=0,
  validation_data = list(X.valid, Y.valid),
  callbacks =  list(
    callback_model_checkpoint(
      filepath = "model2_weights",
      verbose=0, monitor="val_loss",
      save_best_only = TRUE, save_weights_only = TRUE),
      callback_early_stopping(
      monitor = "val_loss", 
      min_delta = 0, patience = 20))
)
```

# Load saved weights

```{r, eval = T}
model2 <- load_model_weights_tf(model2,
                               filepath="model2_weights")
```

# Fitting the model

```{r, eval=TRUE, fig.height = 3, fig.width = 7}

plot(history2)

```

# Comparison

```{r, eval=TRUE}
qhat2 <- model2 %>% predict(X_scaled)
#Sanity check
mean(qhat2[,,1]>= Y)
```

```{r, eval=TRUE}

print(tilted_loss(Y.valid,predict(model,X.valid)[,,1]))
print(tilted_loss(Y.valid,predict(model2,X.valid)[,,1]))
```
Lower validation loss when using the weighted A matrix.

# Predictions

```{r, eval=TRUE, echo = F}
site.ind <-80
par(mfrow=c(1,2))
plot(Y[,site.ind], 
     xlab = "Time",
     ylab = "Hourly rainfall (mm)", 
     pch = 20, 
     col = adjustcolor("black", 0.2))
points(qhat2[,site.ind,],col="red",type="l")
plot(maps::map("world",plot=F),type="l",xlab="",ylab="",ylim=c(42,63),xlim=range(-11,6),pch=20,asp=1/1.6)
points(coords[site.ind,1],coords[site.ind,2],col="red",pch=3)
```

# Predictions

```{r, eval=TRUE, echo =F}
site.ind <-41
par(mfrow=c(1,2))
plot(Y[,site.ind], 
     xlab = "Time",
     ylab = "Hourly rainfall (mm)", 
     pch = 20, 
     col = adjustcolor("black", 0.2))
points(qhat2[,site.ind,],col="red",type="l")
plot(maps::map("world",plot=F),type="l",xlab="",ylab="",ylim=c(42,63),xlim=range(-11,6),pch=20,asp=1/1.6)
points(coords[site.ind,1],coords[site.ind,2],col="red",pch=3)
```

# Estimated quantile maps

```{r, eval=TRUE, echo =F}
par(mfrow=c(1,2))
time.ind <-100
plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
     main = "Hourly rainfall (mm)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(Y[time.ind,])[i]],pch=20)
plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
      main = "Quantile (mm)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(qhat2[time.ind,,])[i]],pch=20)
```


# Estimated quantile maps

```{r, eval=TRUE, echo =F}
par(mfrow=c(1,2))
time.ind <-4000
plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
     main = "Hourly rainfall (mm)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(Y[time.ind,])[i]],pch=20)
plot(maps::map("world",plot=F),type="l",ylab="",ylim=c(44,60),xlim=range(-11,5),pch=20,asp=1/1.6,
      main = "Quantile (mm)", xlab=times[time.ind])
for(i in 1:nrow(coords)) points(coords[i,1],coords[i,2],col=rev(heat.colors(nrow(coords)))[rank(qhat2[time.ind,,])[i]],pch=20)
```

# Graph-level task

Let's switch the problem up a bit. Say I was interested in predicting the probability of having extreme rainfall at this specific location, given all non-rainfall variables.

```{r, eval=TRUE, echo =F}
site.ind <-80
par(mfrow=c(1,2))
plot(Y[,site.ind], 
     xlab = "Time",
     ylab = "Hourly rainfall (mm)", 
     pch = 20, 
     col = adjustcolor("black", 0.2))
plot(maps::map("world",plot=F),type="l",xlab="",ylab="",ylim=c(42,63),xlim=range(-11,6),pch=20,asp=1/1.6)
points(coords[site.ind,1],coords[site.ind,2],col="red",pch=3)
```

# Graph-level task

I will formulate this problem by mapping $Y$ to a binary vector. This now becomes a graph-level classification problem. We have one value for the entire graph!

```{r, eval=TRUE}
site.ind <-80
Y <- 1*as.matrix(
  Y[,site.ind]>quantile(Y[,site.ind],prob=0.9)
  )
dim(Y)
dim(X)

Y.valid <-  as.matrix(Y[validation.inds,])
Y.train <-  as.matrix(Y[-validation.inds,])
```

# Custom pooling layers

We will build a GNN that uses the GCSN layers used before, but also includes some pooling. We will use a global SortPool layer.

```{r, eval=TRUE, echo=T}
layer_Sort_pool <- function(
     object,
     k,
     name=NULL,
     ...)
{
  args <- list(k = as.integer(k),
               name = name
               )
  keras::create_layer(spk$layers$SortPool, object, args)

}
```

# Build model

```{r, eval=T}
input.lay <- layer_input(shape = dim(X)[2:3])
hidden.lay1<- list(input.lay, A2.normed) %>% 
  layer_graph_conv(channels = 32,
                   activation = "relu") 
hidden.lay2<- list(hidden.lay1, A2.normed) %>% 
  layer_graph_conv(channels = 16,
                   activation = "relu")
```

# Pooling layer

```{r, eval=T}
pooled.lay <- hidden.lay2 %>% layer_Sort_pool(k=25)
```

# Flatten and dense layers

```{r, eval=T}

flatten.layer <- pooled.lay %>%
  layer_flatten()
hidden.lay3 <- flatten.layer %>% 
  layer_dense(units=16,
              activation = 'relu')
output.lay <- hidden.lay3 %>% 
  layer_dense(units=1,
              activation = 'sigmoid')
model3 <- keras_model(
    inputs = c(input.lay), 
    outputs = c(output.lay)
  )
```

# Fitting

```{r, eval=T}
model3 %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = c('accuracy')
)
```

# New model fitting

```{r,eval=T}
history <- model3 %>% fit(
  x = X.train, y = Y.train, shuffle=T,
  epochs = 100, batch_size = 32, verbose=0,
  validation_data = list(X.valid, Y.valid),
  callbacks =  list(
    callback_model_checkpoint(
      filepath = "model3_weights",
      verbose=0, monitor="val_loss",
      save_best_only = TRUE, save_weights_only = TRUE),
      callback_early_stopping(
      monitor = "val_loss", 
      min_delta = 0, patience = 10))
)
```

# Load saved weights

```{r, eval = T}
model3 <- load_model_weights_tf(model3,
                               filepath="model3_weights")
```

# Fitting the model

```{r, eval=TRUE, fig.height = 3, fig.width = 7}

plot(history)

```

# Predictions

```{r, eval=TRUE}
phat <- model3 %>% predict(X.valid)

```

Check accuracy

```{r, eval = T}
predictions <- as.numeric(phat %>%
`>`(0.5) %>% k_cast("int32"))
truth <- Y.valid
propCorrect <- sum(predictions == truth)/length(truth)
print(propCorrect)
```
